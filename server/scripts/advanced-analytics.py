"""
Bet Assistant - Advanced Analytics Script
==========================================

Ten skrypt przeprowadza zaawansowanÄ… analizÄ™ danych z typÃ³w bukmacherskich
i tworzy predykcyjny model do okreÅ›lania szans powodzenia.

Wymagania:
    pip install pandas numpy scikit-learn matplotlib seaborn

UÅ¼ycie:
    1. Eksportuj dane: curl http://localhost:3000/api/analytics/export-csv > bets-data.csv
    2. Uruchom: python advanced-analytics.py
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# Konfiguracja
CSV_FILE = 'bets-data.csv'
TEST_SIZE = 0.2
RANDOM_STATE = 42

def load_and_prepare_data(filepath):
    """Wczytaj i przygotuj dane do analizy"""
    print("ğŸ“Š Wczytywanie danych...")
    df = pd.read_csv(filepath, encoding='utf-8-sig')
    
    print(f"âœ… Wczytano {len(df)} rekordÃ³w")
    print(f"ğŸ“‹ Kolumny: {list(df.columns)}")
    
    # Konwersja wyniku na wartoÅ›Ä‡ binarnÄ…
    df['Success'] = (df['WszedÅ‚'].str.lower() == 'tak').astype(int)
    
    # Konwersja procentÃ³w na liczby dziesiÄ™tne
    percent_columns = [col for col in df.columns if '%' in col]
    for col in percent_columns:
        df[col] = df[col].str.rstrip('%').astype(float) / 100
    
    return df

def analyze_correlations(df):
    """Analiza korelacji miÄ™dzy zmiennymi a sukcesem"""
    print("\nğŸ” Analiza korelacji...")
    
    # Wybierz kolumny numeryczne
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    # Oblicz korelacje z sukcesem
    correlations = df[numeric_cols].corrwith(df['Success']).sort_values(ascending=False)
    
    print("\nğŸ“Š Top 10 zmiennych skorelowanych z sukcesem:")
    print(correlations.head(10))
    
    print("\nğŸ“Š Top 10 zmiennych negatywnie skorelowanych:")
    print(correlations.tail(10))
    
    # Wizualizacja
    plt.figure(figsize=(12, 8))
    correlations.drop('Success').plot(kind='barh')
    plt.title('Korelacja zmiennych z sukcesem typu')
    plt.xlabel('Korelacja Pearsona')
    plt.tight_layout()
    plt.savefig('correlations.png', dpi=300, bbox_inches='tight')
    print("\nğŸ’¾ Zapisano wykres: correlations.png")
    
    return correlations

def select_features(df, correlations, threshold=0.1):
    """Wybierz najwaÅ¼niejsze cechy do modelu"""
    print(f"\nğŸ¯ WybÃ³r cech (korelacja > {threshold})...")
    
    # Cechy z wysokÄ… korelacjÄ… (dodatniÄ… lub ujemnÄ…)
    important_features = correlations[abs(correlations) > threshold].index.tolist()
    important_features.remove('Success')  # UsuÅ„ zmiennÄ… docelowÄ…
    
    print(f"âœ… Wybrano {len(important_features)} cech:")
    for feat in important_features:
        print(f"   - {feat}: {correlations[feat]:.3f}")
    
    return important_features

def train_models(X_train, X_test, y_train, y_test):
    """Trenuj i porÃ³wnaj rÃ³Å¼ne modele"""
    print("\nğŸ¤– Trenowanie modeli...")
    
    models = {
        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\nğŸ“ˆ {name}...")
        
        # Trenowanie
        model.fit(X_train, y_train)
        
        # Predykcje
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)[:, 1]
        
        # Metryki
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        auc = roc_auc_score(y_test, y_proba)
        
        # Cross-validation
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
        
        results[name] = {
            'model': model,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        }
        
        print(f"   Accuracy:  {accuracy:.2%}")
        print(f"   Precision: {precision:.2%}")
        print(f"   Recall:    {recall:.2%}")
        print(f"   F1 Score:  {f1:.2%}")
        print(f"   AUC:       {auc:.3f}")
        print(f"   CV Score:  {cv_scores.mean():.2%} (Â±{cv_scores.std():.2%})")
        
        # Confusion Matrix
        cm = confusion_matrix(y_test, y_pred)
        print(f"\n   Confusion Matrix:")
        print(f"   TN: {cm[0,0]:3d}  FP: {cm[0,1]:3d}")
        print(f"   FN: {cm[1,0]:3d}  TP: {cm[1,1]:3d}")
    
    return results

def analyze_feature_importance(model, feature_names):
    """Analiza waÅ¼noÅ›ci cech (dla Random Forest)"""
    if hasattr(model, 'feature_importances_'):
        print("\nğŸ¯ WaÅ¼noÅ›Ä‡ cech (Random Forest):")
        
        importances = pd.DataFrame({
            'feature': feature_names,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(importances.head(10))
        
        # Wizualizacja
        plt.figure(figsize=(12, 8))
        importances.head(15).plot(x='feature', y='importance', kind='barh')
        plt.title('WaÅ¼noÅ›Ä‡ cech w modelu Random Forest')
        plt.xlabel('WaÅ¼noÅ›Ä‡')
        plt.tight_layout()
        plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
        print("\nğŸ’¾ Zapisano wykres: feature_importance.png")
        
        return importances

def generate_weights_formula(correlations, features):
    """Wygeneruj formuÅ‚Ä™ z wagami dla kaÅ¼dej cechy"""
    print("\nğŸ“ Sugerowana formuÅ‚a obliczania szans:")
    print("\nszanse = (")
    
    # Normalizuj korelacje do sum = 1
    abs_correlations = abs(correlations[features])
    normalized_weights = abs_correlations / abs_correlations.sum()
    
    for i, (feat, weight) in enumerate(normalized_weights.items()):
        operator = "+" if i == 0 else "  +"
        print(f"  {operator} {weight:.4f} Ã— {feat}")
    
    print(") Ã— 100%")
    
    # Zapisz wagi do pliku JSON
    weights_dict = normalized_weights.to_dict()
    
    import json
    with open('model_weights.json', 'w', encoding='utf-8') as f:
        json.dump(weights_dict, f, indent=2, ensure_ascii=False)
    
    print("\nğŸ’¾ Zapisano wagi do: model_weights.json")
    
    return weights_dict

def analyze_by_bet_type(df):
    """Analiza skutecznoÅ›ci wedÅ‚ug typu zakÅ‚adu"""
    print("\nğŸ“Š Analiza wedÅ‚ug typu zakÅ‚adu:")
    
    bet_stats = df.groupby('ZakÅ‚ad').agg({
        'Success': ['count', 'sum', 'mean']
    }).round(3)
    
    bet_stats.columns = ['Liczba', 'Trafione', 'SkutecznoÅ›Ä‡']
    bet_stats['SkutecznoÅ›Ä‡'] = bet_stats['SkutecznoÅ›Ä‡'] * 100
    bet_stats = bet_stats.sort_values('SkutecznoÅ›Ä‡', ascending=False)
    
    print(bet_stats)
    
    # Wizualizacja
    if len(bet_stats) > 0:
        plt.figure(figsize=(12, 6))
        bet_stats['SkutecznoÅ›Ä‡'].plot(kind='bar')
        plt.title('SkutecznoÅ›Ä‡ wedÅ‚ug typu zakÅ‚adu')
        plt.ylabel('SkutecznoÅ›Ä‡ (%)')
        plt.xlabel('Typ zakÅ‚adu')
        plt.xticks(rotation=45, ha='right')
        plt.axhline(y=50, color='r', linestyle='--', label='50% (prÃ³g)')
        plt.legend()
        plt.tight_layout()
        plt.savefig('bet_type_performance.png', dpi=300, bbox_inches='tight')
        print("\nğŸ’¾ Zapisano wykres: bet_type_performance.png")

def analyze_by_league(df):
    """Analiza skutecznoÅ›ci wedÅ‚ug ligi"""
    print("\nğŸ† Analiza wedÅ‚ug ligi:")
    
    league_stats = df.groupby('Liga').agg({
        'Success': ['count', 'sum', 'mean']
    }).round(3)
    
    league_stats.columns = ['Liczba', 'Trafione', 'SkutecznoÅ›Ä‡']
    league_stats['SkutecznoÅ›Ä‡'] = league_stats['SkutecznoÅ›Ä‡'] * 100
    league_stats = league_stats[league_stats['Liczba'] >= 5]  # Min 5 typÃ³w
    league_stats = league_stats.sort_values('SkutecznoÅ›Ä‡', ascending=False)
    
    print(league_stats.head(15))

def analyze_odds_impact(df):
    """Analiza wpÅ‚ywu kursu na skutecznoÅ›Ä‡"""
    print("\nğŸ’° Analiza wpÅ‚ywu kursu:")
    
    df['OddsRange'] = pd.cut(df['Kurs'], bins=[0, 1.5, 2, 2.5, 3, 100], 
                              labels=['< 1.5', '1.5-2.0', '2.0-2.5', '2.5-3.0', 'â‰¥ 3.0'])
    
    odds_stats = df.groupby('OddsRange').agg({
        'Success': ['count', 'sum', 'mean']
    }).round(3)
    
    odds_stats.columns = ['Liczba', 'Trafione', 'SkutecznoÅ›Ä‡']
    odds_stats['SkutecznoÅ›Ä‡'] = odds_stats['SkutecznoÅ›Ä‡'] * 100
    
    print(odds_stats)

def main():
    """GÅ‚Ã³wna funkcja analizy"""
    print("=" * 60)
    print("ğŸ° Bet Assistant - Zaawansowana Analityka")
    print("=" * 60)
    
    # 1. Wczytaj dane
    df = load_and_prepare_data(CSV_FILE)
    
    # 2. Podstawowe statystyki
    print(f"\nğŸ“Š Podstawowe statystyki:")
    print(f"   Wszystkie typy:  {len(df)}")
    print(f"   Trafione:        {df['Success'].sum()} ({df['Success'].mean()*100:.1f}%)")
    print(f"   Nietrafione:     {len(df) - df['Success'].sum()} ({(1-df['Success'].mean())*100:.1f}%)")
    
    # 3. Analiza korelacji
    correlations = analyze_correlations(df)
    
    # 4. WybÃ³r cech
    features = select_features(df, correlations, threshold=0.1)
    
    # Przygotuj dane do treningu
    X = df[features].fillna(df[features].mean())
    y = df['Success']
    
    # Normalizacja
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled = pd.DataFrame(X_scaled, columns=features)
    
    # PodziaÅ‚ train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
    )
    
    print(f"\nâœ‚ï¸  PodziaÅ‚ danych:")
    print(f"   Trening: {len(X_train)} ({len(X_train)/len(df)*100:.1f}%)")
    print(f"   Test:    {len(X_test)} ({len(X_test)/len(df)*100:.1f}%)")
    
    # 5. Trenowanie modeli
    results = train_models(X_train, X_test, y_train, y_test)
    
    # 6. Analiza waÅ¼noÅ›ci cech
    best_model = results['Random Forest']['model']
    analyze_feature_importance(best_model, features)
    
    # 7. Generuj formuÅ‚Ä™
    weights = generate_weights_formula(correlations, features)
    
    # 8. Dodatkowe analizy
    analyze_by_bet_type(df)
    analyze_by_league(df)
    analyze_odds_impact(df)
    
    # 9. Podsumowanie
    print("\n" + "=" * 60)
    print("âœ… Analiza zakoÅ„czona!")
    print("=" * 60)
    print("\nğŸ“ Wygenerowane pliki:")
    print("   - correlations.png          (wykres korelacji)")
    print("   - feature_importance.png    (waÅ¼noÅ›Ä‡ cech)")
    print("   - bet_type_performance.png  (skutecznoÅ›Ä‡ typÃ³w)")
    print("   - model_weights.json        (wagi do implementacji)")
    
    print("\nğŸ¯ Najlepszy model:")
    best_name = max(results.items(), key=lambda x: x[1]['f1'])[0]
    best_results = results[best_name]
    print(f"   {best_name}")
    print(f"   Accuracy:  {best_results['accuracy']:.2%}")
    print(f"   F1 Score:  {best_results['f1']:.2%}")
    print(f"   AUC:       {best_results['auc']:.3f}")
    
    print("\nğŸ’¡ NastÄ™pne kroki:")
    print("   1. Zaimplementuj wagi z model_weights.json w kodzie")
    print("   2. Testuj nowy wzÃ³r na nowych danych")
    print("   3. Zbieraj wiÄ™cej danych i przeliczy model co ~50-100 typÃ³w")
    print("   4. Monitoruj ROI i dostosowuj wagi w razie potrzeby")

if __name__ == '__main__':
    main()
